{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd6f0b73-8ac7-4181-9f2a-1b585df47e87",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Assignment 3\n",
    "\n",
    "##### Deliar Mohammadi, 30072994\n",
    "##### John Zheng,30125258\n",
    "##### Xinzhou Li, 30066080\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0819318d-5b5a-481b-9458-d4e7f0bde01c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch_optimizer as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4269e9d8-2d49-42cc-885c-f04ea3dadbac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Building Train and Test Data classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "95b871f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 37 41 43 43 44 41 43 41 46 42 43 40 45 40 34 36 41 43 42 44 42 42 45 41 41 39 41 37 39 36 35 46 45 33 44 35 43 38 42 43 41 45 40 40 47 38 44 45 37 43 37 44 30 45 42 39 40 38 43 35 45 45 44 44 44 41 38 49 43 45 45 41 42 44 43 44 40 44 41 39 40 39 44 46 41 42 46 28 34 42 39 40 43 44 42 45 44 45 43\n"
     ]
    }
   ],
   "source": [
    "df_csv = pd.DataFrame(pd.read_csv(\"./Assignment 3 Dataset/train.csv\"))\n",
    "print(' '.join([str((df_csv['label'] == i).sum()) for i in range(100)]))\n",
    "\n",
    "train_paths = []\n",
    "train_labels = []\n",
    "validation_paths = []\n",
    "validation_labels = []\n",
    "label_counts = [0 for _ in range(100)]\n",
    "\n",
    "for i in range(len(df_csv)):\n",
    "    l = df_csv['label'].iat[i]\n",
    "    if label_counts[l] < 1:\n",
    "        validation_paths.append(df_csv['id'].iat[i])\n",
    "        validation_labels.append(l)\n",
    "        label_counts[l] += 1\n",
    "    else:\n",
    "        train_paths.append(df_csv['id'].iat[i])\n",
    "        train_labels.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a07f542e-ed26-4bd5-94c5-ca1eaa6ee40b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self, root, paths, labels, transform=None, ext='.jpg'): # move transform into training loop\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.root = root\n",
    "        # print(self.all_file_paths)\n",
    "        # print(self.all_labels)\n",
    "        \n",
    "    '''\n",
    "    return length of the dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    '''\n",
    "    get the img,label tuple corresponding to index i\n",
    "    '''\n",
    "    def __getitem__(self, i):\n",
    "        s = (self.root+'/'+self.paths[i])\n",
    "        img = Image.open(s)\n",
    "        label = self.labels[i]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "     \n",
    "    '''\n",
    "    get the img,label tuple corresponding to index i\n",
    "    '''\n",
    "    def get_no_transform(self, i):\n",
    "        s = (self.root+'/'+self.paths[i])\n",
    "        img = Image.open(s)\n",
    "        label = self.labels[i]\n",
    "        return img, label\n",
    "    \n",
    "    '''\n",
    "    Selects random start point in the dataset and prints 28 images\n",
    "    Inputs:\n",
    "        color-> 1 to print color images, 0 for grayscale images\n",
    "    '''\n",
    "    def print_samples(self):\n",
    "        figure, axes = plt.subplots(3, 7, figsize=(18,10))\n",
    "        axes = axes.flatten()\n",
    "        i = random.randint(0, len(self)-29)\n",
    "        \n",
    "        for axis in axes:\n",
    "            x = self.get_no_transform(i)\n",
    "            axis.imshow(x[0])\n",
    "            label = x[1]\n",
    "            axis.set_xlabel(label)\n",
    "            i+=1\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    '''\n",
    "    Calculates average width and height of all images in dataset\n",
    "    '''\n",
    "    def calc_average_dimension(self):\n",
    "        totalw = 0\n",
    "        totalh = 0\n",
    "        for i in range(len(self.paths)):\n",
    "            image = (self.get_no_transform(i))[0]\n",
    "            w, h = image.size\n",
    "            totalw += w\n",
    "            totalh += h\n",
    "        avgw = totalw//len(self)\n",
    "        avgh = totalh//len(self)\n",
    "        return avgw, avgh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907a2fa-8ccc-40f7-9105-c07eec2344a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing Training & Validation datasets, dataloaders and transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "21ed6657-4a7e-40bc-a10a-159442f2655e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=(-20, 20)),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.4),\n",
    "    transforms.ColorJitter(brightness=0.2),\n",
    "    transforms.ColorJitter(contrast=0.5),\n",
    "    transforms.ColorJitter(saturation=0.5),\n",
    "    \n",
    "    transforms.Resize(size=256),     \n",
    "    torchvision.transforms.CenterCrop(224), #required for resnet50 input: 224x224\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # normalized with imagenet mean and std deviation\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8109165c-5250-4021-9db7-ed3a4fe7fedc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_transform = transforms.Compose([\n",
    "    transforms.Resize(size=256),     \n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cf6bc036-50fb-4f6c-96a7-f5bfb7f44c82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Data(\"./Assignment 3 Dataset\", train_paths, train_labels, train_transform)\n",
    "validation_dataset = Data(\"./Assignment 3 Dataset\", validation_paths, validation_labels, validation_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "644e7aab-9924-43fb-9229-f7a8298fcf09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac0e2b-51aa-4592-a205-2ab225249c32",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Sampels of training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1d86a3-fa7a-4fb4-af85-dc035bc28fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_dataset.print_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f08bb6-3dd5-4759-8303-ae91e70cc5f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building the Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a3e5fc75-4aed-4c2e-93a3-54506fb1d984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class A3ResNetModel():\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        \n",
    "        self.model = models.resnet50(weights=True).to(self.device)\n",
    "        self.freeze_features()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 100)\n",
    "        )\n",
    "        \n",
    "        self.model.fc = self.fc\n",
    "        self.model.to(self.device)\n",
    "    \n",
    "    def freeze_features(self):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False \n",
    "    \n",
    "    def print_model(self):\n",
    "        summary(self.model, (3,224,224))\n",
    "\n",
    "        \n",
    "    def train_model(self, epochs, train_dataloader, optimizer, loss_function):\n",
    "        self.model.train()\n",
    "        for i in tqdm(range(epochs)):\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            num_examples = 0\n",
    "            correct_examples = 0\n",
    "            \n",
    "            for batch_index,(images,labels) in tqdm(enumerate(train_dataloader)):\n",
    "                # reset optimizer for the current batch\n",
    "                optimizer.zero_grad() \n",
    "                # load images and labels\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # calculate output and loss from output\n",
    "                outputs = self.model(images)\n",
    "                loss = loss_function(outputs,labels)\n",
    "                # loss.backward() calculates all gradients for all params, optimizer.step() updates params using our optimizer\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # calculate information for this epoch\n",
    "                epoch_loss += loss.item()*len(labels)\n",
    "                num_examples += len(labels)\n",
    "                # calculate predictions and accuracy\n",
    "                _,preds = torch.max(outputs,1) # calcualtae max in each column (second value=1 for column)\n",
    "                correct_examples += (preds==labels).sum().item()\n",
    "                \n",
    "            # AFTER epoch we calculate accuracy\n",
    "            train_accuracy = (correct_examples/num_examples)*100\n",
    "            print(f'Epoch {i}: Training Accuracy: {train_accuracy:.4f}%, Training Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            \n",
    "    def validate_model(self, validation_dataloader):\n",
    "        self.model.eval() \n",
    "        num_examples = 0\n",
    "        correct_examples = 0\n",
    "        \n",
    "        for batch_index,(images,labels) in tqdm(enumerate(validation_dataloader)):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = self.model(images)\n",
    "            num_examples += len(labels)\n",
    "            _ , preds = torch.max(outputs,1)\n",
    "            correct_examples += (preds==labels).sum().item()\n",
    "            \n",
    "        validation_accuracy = (correct_examples / num_examples)*100\n",
    "        print(f'Validation Accuracy: {validation_accuracy:.20f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "36bf4f3c-9d6a-45fe-bf7c-f41942befa08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0214bb7f-f6b3-41d4-8b93-fe1ad6530649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resnet50 = A3ResNetModel(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0974dff3-c74a-4119-9f7a-7bdf9154be6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet50.model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c9b74651-e89f-445b-bd87-11a60de982eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127it [07:58,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Training Accuracy: 0.9913%, Training Loss: 19746.6287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "resnet50.train_model(1, train_dataloader, optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e74674e6-0228-44a2-bc2a-fc5ce99fa4b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:10,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 1.00000000000000000000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "resnet50.validate_model(validation_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb5b22-7b85-448f-b094-0c944da4c6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49623231-9abb-457a-9c0d-e75ea33fedaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "165a6394-0775-4735-a2f5-cea555f2dc95",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "https://pytorch.org/vision/stable/models.html\n",
    "\n",
    "https://discuss.pytorch.org/t/what-does-it-mean-to-normalize-images-for-resnet/96160"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
